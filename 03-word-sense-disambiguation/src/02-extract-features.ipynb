{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "\n",
    "At this point, we have **manually annotated** 19 sentences with the word `compound` used in the chemical-compound sense, and 21 sentences with the word `compound` used in the multiple/composite sense. These have been stored as an additional column in the input file.\n",
    "\n",
    "The paper states that the Label Propagation solution works well when the manual annotations cover about 10% of the dataset. In our case, this is approximately 5% of the dataset.\n",
    "\n",
    "In this notebook, we build feature vectors for each sentence using features described in the paper as follows.\n",
    "\n",
    "    We used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations\n",
    "\n",
    "This translates into the following features for our implementation.\n",
    "\n",
    "* TF-IDF vectors for terms, bigrams, and trigrams.\n",
    "* TF-IDF vectors for part of speech trigrams.\n",
    "\n",
    "We then compute the sentence-sentence similarity matrix using cosine similarity, this is the initial version of our graph adjacency matrix.\n",
    "\n",
    "We then sparsify the graph by discarding all but the top k (k=5) similar sentences for each, and finally remove any self edges by zero-ing out the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SENTS_FILEPATH = os.path.join(DATA_DIR, \"sentences-compound-plabels.tsv\")\n",
    "\n",
    "SIM_MATRIX_FILEPATH = os.path.join(DATA_DIR, \"sim-matrix.npy\")\n",
    "\n",
    "NUM_EDGES_TO_KEEP = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "\n",
    "Declare some things that we will use multiple times later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punct_pattern = \"|\".join([\"\\\\\" + c for c in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean input text\n",
    "\n",
    "Excel went ahead and randomly decided to enclose some of the sentences in quotes. So the cleaning of input text consists of conditionally removing the enclosing quotes, lowercasing the text, and replacing punctuation with space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two of the case subjects were compound heterozygous including for a variant observed in six control subjects and one was homozygous \n"
     ]
    }
   ],
   "source": [
    "def clean_text(sent_text, punct_pattern):\n",
    "    # remove bounding quotes\n",
    "    if sent_text.startswith(\"\\\"\") and sent_text.endswith(\"\\\"\"):\n",
    "        sent_text = sent_text[1:-1]\n",
    "    # lowercase sentence\n",
    "    sent_text = sent_text.lower()\n",
    "    # replace punctuations with space\n",
    "    sent_text = re.sub(punct_pattern, \" \", sent_text)\n",
    "    sent_text = re.sub(\"\\s+\", \" \", sent_text)\n",
    "    return sent_text\n",
    "\n",
    "\n",
    "# test\n",
    "s = \"\\\"Two of the case subjects were compound heterozygous, including for a variant observed in six control subjects, and one was homozygous.\\\"\"\n",
    "print(clean_text(s, punct_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting POS tags\n",
    "\n",
    "For each sentence, we will create the corresponding POS tag sequence as a string. This string will be passed into the Scikit-learn vectorizer similar to the sentence text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT NUM ADP DET NOUN NOUN VERB ADJ ADJ PUNCT VERB ADP DET NOUN VERB ADP NUM NOUN NOUN PUNCT CCONJ NUM VERB ADJ PUNCT PUNCT\n"
     ]
    }
   ],
   "source": [
    "def extract_pos_tags(sent_text, nlp):\n",
    "    pos_tags = []\n",
    "    doc = nlp(sent_text)\n",
    "    for token in doc:\n",
    "        pos_tags.append(token.pos_)\n",
    "    # return POS tags as a string\n",
    "    return \" \".join(pos_tags)\n",
    "\n",
    "\n",
    "# test\n",
    "print(extract_pos_tags(s, nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text and POS sequences\n",
    "\n",
    "For each sentence, we create TF-IDF vectors corresponding to the token n-grams (for n=1..3) and POS 3-grams. These two vectors are concatenated and represent the combined feature vector for the sentence.\n",
    "\n",
    "We use POS tokens here since we are looking at Word Sense Disambiguation, and the extra hints provided by POS tags are generally useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(texts, ngram_range):\n",
    "    vec = TfidfVectorizer(ngram_range=ngram_range, \n",
    "        use_idf=True, min_df=5, max_df=0.2,\n",
    "        norm=\"l2\")\n",
    "    td_matrix = vec.fit_transform(texts)\n",
    "    return td_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentences read\n",
      "500 sentences read\n",
      "668 sentences read, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "poss, texts = [], []\n",
    "num_read = 0\n",
    "fsents = open(SENTS_FILEPATH, \"r\")\n",
    "for line in fsents:\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "    if num_read % 500 == 0:\n",
    "        print(\"{:d} sentences read\".format(num_read))\n",
    "    pii, sent_id, sent_text, label = line.strip().split('\\t')\n",
    "    label = int(label)\n",
    "    poss.append(extract_pos_tags(sent_text, nlp))\n",
    "    texts.append(clean_text(sent_text, punct_pattern))\n",
    "    num_read += 1\n",
    "\n",
    "print(\"{:d} sentences read, COMPLETE\".format(num_read))\n",
    "fsents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 902) (668, 499) (668, 1401)\n"
     ]
    }
   ],
   "source": [
    "text_matrix = tfidf_vectorize(texts, ngram_range=(1, 3))\n",
    "pos_matrix = tfidf_vectorize(poss, ngram_range=(3, 3))\n",
    "feat_matrix = np.hstack((text_matrix, pos_matrix))\n",
    "print(text_matrix.shape, pos_matrix.shape, feat_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Similarity Matrix\n",
    "\n",
    "The similarity matrix uses cosine similarities. Final form of the similarity matrix is the graph adjacency matrix that we will run Label Propagation against.\n",
    "\n",
    "Similarity matrix is normalized so all diagonal elements (cosine similarity for sentence with itself) is always 1.\n",
    "\n",
    "We also remove the ability for any self-traversals by zero-izing the diagonal elements.\n",
    "\n",
    "Then, in order to sparsify the graph, for all sentences, we discard all but the top 5 edges to neighboring sentences.\n",
    "\n",
    "Finally, the resulting graph adjacency matrix is saved in order to be used by the next step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 668)\n"
     ]
    }
   ],
   "source": [
    "S = np.matmul(feat_matrix, feat_matrix.T)\n",
    "S /= np.diag(S)[0]\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard self edges, i.e., diagonal = 0\n",
    "for i in range(S.shape[0]):\n",
    "    S[i, i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard all but top N edges from the adjacency matrix\n",
    "num_to_discard = S.shape[1] - NUM_EDGES_TO_KEEP\n",
    "zero_indices = np.argpartition(S, -NUM_EDGES_TO_KEEP, axis=1)[:, 0:num_to_discard]\n",
    "for i in range(zero_indices.shape[0]):\n",
    "    for j in zero_indices[i]:\n",
    "        S[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(SIM_MATRIX_FILEPATH, S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
