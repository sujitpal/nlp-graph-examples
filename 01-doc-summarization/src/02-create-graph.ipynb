{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph\n",
    "\n",
    "For the summarization task, we model a document as a graph of sentence nodes. Each node represents a single sentence from our `sentences.txt` file.\n",
    "\n",
    "Nodes are connected to each other by edges that represent co-occurring commonly occurring nouns. Consider two sentences `s1` and `s2`. If they share a noun, we draw an edge between them with weight 1. If they share two nouns between them, the edge between them has weight 2, and so on.\n",
    "\n",
    "In this notebook, we will process each sentence to extract their nouns, compute similarity between all pairs of sentences based on noun co-occurrence, and generate CSV files that will be used by `neo4j-admin` to load the graph into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SENTENCE_PATH = os.path.join(DATA_DIR, \"sentences.txt\")\n",
    "\n",
    "NODE_PATH = os.path.join(DATA_DIR, \"nodes.csv\")\n",
    "EDGE_PATH = os.path.join(DATA_DIR, \"edges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract noun phrases\n",
    "\n",
    "We will use models from the Spacy English model to tokenize each sentence in our `sentences.txt` file, POS tag them and extract only nouns, lemmatize the nouns, and create a vocabulary of nouns in our document.\n",
    "\n",
    "Lemmatization is done so we can treat words such as `countries` and `country` the same way. Clearly a sentence that mentions the first variation of the word is similar to another that mentions the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ctr = collections.Counter()\n",
    "num_docs = 0\n",
    "fsents = open(SENTENCE_PATH, \"r\")\n",
    "for line in fsents:\n",
    "    line = line.strip()\n",
    "    doc = nlp(line)\n",
    "    for token in doc:\n",
    "        if token.tag_.startswith(\"NN\"):\n",
    "            word = token.text.lower()\n",
    "            lemma = token.lemma_\n",
    "            noun_ctr[lemma] += 1\n",
    "    num_docs += 1\n",
    "\n",
    "fsents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iran', 41),\n",
       " ('u.s.', 14),\n",
       " ('country', 12),\n",
       " ('america', 11),\n",
       " ('united', 11),\n",
       " ('states', 11),\n",
       " ('venezuela', 10),\n",
       " ('sanction', 10),\n",
       " ('ahmadinejad', 9),\n",
       " ('latin', 9)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_ctr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard \"uncommon\" nouns\n",
    "\n",
    "We will discard nouns which occur only once. It is also reasonable to use a higher threshold depending on the richness of the vocabulary extracted.\n",
    "\n",
    "The `vocab` dictionary is used to map each common noun to a position, so we can represent each document as a vector of common nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nouns = [x[0] for x in noun_ctr.most_common() if x[1] > 1]\n",
    "vocab = {x: i for (i, x) in enumerate(common_nouns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Term-Document Matrix\n",
    "\n",
    "We will represent each document as a vector of (lemmas of) common nouns. For that, we will construct a Term Document (TD) Matrix first, which is initially populated with zeros.\n",
    "\n",
    "We populate the TD Matrix by reading through the `sentence.txt` file again, line by line, this time matching the tokens against our vocabulary represented by the `vocab` dictionary. For each common noun, we will add 1 to the TD matrix at the position for that document and noun position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = np.zeros((num_docs, len(common_nouns)))\n",
    "\n",
    "num_docs = 0\n",
    "fsents = open(SENTENCE_PATH, \"r\")\n",
    "for line in fsents:\n",
    "    line = line.strip()\n",
    "    doc = nlp(line)\n",
    "    for token in doc:\n",
    "        if token.lemma_ in vocab.keys():\n",
    "            td_matrix[num_docs, vocab[token.lemma_]] += 1\n",
    "    num_docs += 1\n",
    "\n",
    "fsents.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Similarity Matrix\n",
    "\n",
    "The shape of the TD Matrix is (number of sentences, number of common nouns). \n",
    "\n",
    "We want to construct a matrix that will contain similarities between sentences in terms of common noun co-occurrence, i.e., a matrix of shape (number of sentences, number of sentences).\n",
    "\n",
    "This can be done by matrix multiplying the TD Matrix of shape (number of sentences, number of common nouns) with its transpose of shape (number of common nouns, number of sentences).\n",
    "\n",
    "Normalization (as is done in case of cosine similarity) is not required, since we are really concerned about edge weights relative to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n"
     ]
    }
   ],
   "source": [
    "S = np.matmul(td_matrix, td_matrix.T)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files to load into Neo4j\n",
    "\n",
    "We will create two CSV files, one containing nodes, and the other containing edges, to be used by the `neo4j-admin` tool, to bulk import our graph into Neo4j.\n",
    "\n",
    "The `neo4j-admin` tool is able to parse the schema from the CSV header line. \n",
    "\n",
    "Our nodes just contain the sentence ID (`sid`) and the entity label `Sentence`. For our `sid`, we just use `s` followed by a zero padded 3-digit running number (Padding is needed since our algorithm will attempt to sort by position -- we probably should have specified a numeric position attribute and used that instead, but this works as well). The first few lines of `nodes.csv` looks like this:\n",
    "\n",
    "    sid:ID,:LABEL\n",
    "    s000,Sentence\n",
    "    s001,Sentence\n",
    "    ...\n",
    "\n",
    "Our relationships contain the start `sid`, the similarity value `sim`, the end `sid`, and the relationship type `SIM`. The similarity value corresponds to the value extracted from the similarity matrix `S` we generated earlier. First few lines of `edges.csv` look like this:\n",
    "\n",
    "    :START_ID,sim:int,:END_ID,:TYPE\n",
    "    s000,1,s001,SIM\n",
    "    s000,1,s002,SIM\n",
    "\n",
    "Note that we have used `sim:int` to indicate that Neo4j should consider the second column as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 80\n",
      "number of edges: 2400\n"
     ]
    }
   ],
   "source": [
    "num_nodes, num_edges = 0, 0\n",
    "\n",
    "fnodes = open(NODE_PATH, \"w\")\n",
    "fnodes.write(\"sid:ID,:LABEL\\n\")\n",
    "for i in range(S.shape[0]):\n",
    "    fnodes.write(\"s{:03d},Sentence\\n\".format(i))\n",
    "    num_nodes += 1\n",
    "\n",
    "print(\"number of nodes: {:d}\".format(num_nodes))\n",
    "fnodes.close()\n",
    "\n",
    "fedges = open(EDGE_PATH, \"w\")\n",
    "fedges.write(\":START_ID,sim:int,:END_ID,:TYPE\\n\")\n",
    "for i in range(S.shape[0]):\n",
    "    for j in range(S.shape[1]):\n",
    "        if S[i, j] > 0 and i != j:\n",
    "            fedges.write(\"s{:03d},{:d},s{:03d},SIM\\n\".format(i, int(S[i,j]), j))\n",
    "            num_edges += 1\n",
    "print(\"number of edges: {:d}\".format(num_edges))\n",
    "fedges.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Load Graph into Neo4j\n",
    "\n",
    "We will use the neo4j-admin tool to bulk load data from the generated CSV files `nodes.csv` and `edges.csv`. This is achieved by the following command.\n",
    "\n",
    "    cd $NEO4J_HOME\n",
    "    bin/neo4j-admin import \\\n",
    "        --nodes=/path/to/nodes.csv \\\n",
    "        --relationships=/path/to/edges.csv\n",
    "\n",
    "Note that the neo4j server should be stopped during this time, or `neo4j-admin` complains about other users using the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
