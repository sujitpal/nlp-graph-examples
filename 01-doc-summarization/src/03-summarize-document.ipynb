{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Document\n",
    "\n",
    "We have converted the input document to a Neo4j graph, where nodes correspond to sentences, and edges connect sentences that share common meaningful nouns.\n",
    "\n",
    "Our objective is to generate a fixed-length (number of sentences) extractive summary of the document.\n",
    "\n",
    "In this notebook, we will apply various graph metrics and techniques to build different summarizers. We then use a voting summarizer that reports, for each sentence of the summary, the sentence that has been recommended by the maximum number of summarizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import py2neo\n",
    "import operator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SENTENCE_PATH = os.path.join(DATA_DIR, \"sentences.txt\")\n",
    "\n",
    "NEO4J_CONN_URL = \"bolt://localhost:7687\"\n",
    "\n",
    "NUM_SENTS_IN_SUMMARY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Our graph does not contain the sentence text, so we need to build an external index that returns the sentence text given the index. The `build_sentence_index` creates a dictionary of sentence index to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_index(sentence_file):\n",
    "    sent_index = {}\n",
    "    fsents = open(sentence_file, \"r\")\n",
    "    for lid, line in enumerate(fsents):\n",
    "        sent_index[\"s{:03d}\".format(lid)] = line.strip()\n",
    "    fsents.close()\n",
    "    return sent_index\n",
    "\n",
    "sent_index = build_sentence_index(os.path.join(DATA_DIR, \"sentences.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `display_recommendations` is a convenience function that returns the recommendations for the top positions from a particular summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(sent_index, label, sids, debug=True):\n",
    "    if debug:\n",
    "        print(\"%10s: %s\" % (label, str(sids)))\n",
    "    else:\n",
    "        print(\"%10s: %s\" % (label, sids))\n",
    "        for sid in sids:\n",
    "            print(sid, \":\", sent_index[sid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = py2neo.Graph(NEO4J_CONN_URL, auth=(\"neo4j\", \"graph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Summarizer (CN-Degree)\n",
    "\n",
    "Summarization involves identifying \"important\" sentences in the document, and degree centrality gives an indication of how connected it is to other sentences. Hence this metric is used in this summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    degree: ['s005', 's021', 's056', 's062', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_degree_centrality(graph):\n",
    "    query = \"\"\"\n",
    "        CALL algo.degree.stream(\"Sentence\", \"SIM\", {direction: \"both\"})\n",
    "        YIELD nodeId, score\n",
    "        RETURN algo.asNode(nodeId).sid AS sid, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT %d\n",
    "    \"\"\" % (NUM_SENTS_IN_SUMMARY)\n",
    "    results = graph.run(query).data()\n",
    "    recos = [x[\"sid\"] for x in results]\n",
    "    recos = sorted(recos)\n",
    "    return recos\n",
    "\n",
    "degree_recos = compute_degree_centrality(graph)\n",
    "display_recommendations(sent_index, \"degree\", degree_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength Summarizer (CN-Strength)\n",
    "\n",
    "Degree centrality is just based only on the number of edges incident upon a node. Strength centrality is basically degree centrality but considering the weights of the edges as well. Similar to degree centrality, this measure can also be thought of as a useful proxy for importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  strength: ['s008', 's021', 's056', 's062', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_strength_centrality(graph):\n",
    "    query = \"\"\"\n",
    "        CALL algo.degree.stream(\"Sentence\", \"SIM\", {\n",
    "            direction: \"both\", weightProperty: \"sim\"})\n",
    "        YIELD nodeId, score\n",
    "        RETURN algo.asNode(nodeId).sid AS sid, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT %d\n",
    "    \"\"\" % (NUM_SENTS_IN_SUMMARY)\n",
    "    results = graph.run(query).data()\n",
    "    recos = [x[\"sid\"] for x in results]\n",
    "    recos = sorted(recos)\n",
    "    return recos\n",
    "\n",
    "strength_recos = compute_strength_centrality(graph)\n",
    "display_recommendations(sent_index, \"strength\", strength_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closeness Summarizer (CN-LI)\n",
    "\n",
    "High values of closeness centrality indicates nodes that are able to spread information across the network more efficiently. This roughly corresponds to CN-LI (Locality Index summarizer) of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " closeness: ['s005', 's021', 's056', 's062', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_closeness_centrality(graph):\n",
    "    query = \"\"\"\n",
    "        CALL algo.closeness.stream('Sentence', 'SIM')\n",
    "        YIELD nodeId, centrality\n",
    "        RETURN algo.asNode(nodeId).sid AS sid, centrality AS score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT %d\n",
    "    \"\"\" % (NUM_SENTS_IN_SUMMARY)\n",
    "    results = graph.run(query).data()\n",
    "    recos = [x[\"sid\"] for x in results]\n",
    "    recos = sorted(recos)\n",
    "    return recos\n",
    "\n",
    "closeness_recos = compute_closeness_centrality(graph)\n",
    "display_recommendations(sent_index, \"closeness\", closeness_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank Summarizer (CN-PR)\n",
    "\n",
    "PageRank is a well-known measure of centrality and is hence included here. The paper did not use this measure, but we think it is important enough to include in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pagerank: ['s005', 's021', 's056', 's062', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_pagerank_centrality(graph):\n",
    "    query = \"\"\"\n",
    "        CALL algo.pageRank.stream('Sentence', 'SIM', \n",
    "            {iterations:20, dampingFactor:0.85})\n",
    "        YIELD nodeId, score\n",
    "        RETURN algo.asNode(nodeId).sid AS sid, score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT %d\n",
    "    \"\"\" % (NUM_SENTS_IN_SUMMARY)\n",
    "    results = graph.run(query).data()\n",
    "    recos = [x[\"sid\"] for x in results]\n",
    "    recos = sorted(recos)\n",
    "    return recos\n",
    "\n",
    "pagerank_recos = compute_pagerank_centrality(graph)\n",
    "display_recommendations(sent_index, \"pagerank\", pagerank_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path Summarizer (CN-SP)\n",
    "\n",
    "Sentences that are close to other sentences in the graph are likely to be good candidates for summary sentences. This technique computes the shortest paths between all pairs of nodes, then computes the mean shortest path from each node. The ones with low mean shortest paths are preferred for summary purposes. The corresponds to the CN-SP summarizer in the paper.\n",
    "\n",
    "Two variants of CN-SP have been proposed, first by normalizing the non-zero weights, and the second by computing reciprocals of non-zero weights of the shortest path matrix. These correspond to the CN-SP<sup>WC</sup> and CN-SP<sup>WI</sup> in the paper respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MSP: ['s010', 's058', 's017', 's051', 's024']\n",
      "     MSP_n: ['s071', 's067', 's078', 's041', 's050']\n",
      "     MSP_r: ['s071', 's067', 's078', 's041', 's050']\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_shortest_path(graph, preprocess_weight=None):\n",
    "    query = \"\"\"\n",
    "        CALL algo.allShortestPaths.stream('sim', {nodeQuery:'Sentence',defaultValue:1.0})\n",
    "        YIELD sourceNodeId, targetNodeId, distance\n",
    "        WITH sourceNodeId, targetNodeId, distance\n",
    "        WHERE algo.isFinite(distance) = true\n",
    "        MATCH (source:Sentence) WHERE id(source) = sourceNodeId\n",
    "        MATCH (target:Sentence) WHERE id(target) = targetNodeId\n",
    "        WITH source, target, distance WHERE source <> target\n",
    "        RETURN source.sid AS source, target.sid AS target, distance\n",
    "    \"\"\"\n",
    "    results_df = graph.run(query).to_data_frame()\n",
    "    if preprocess_weight is not None:\n",
    "        if preprocess_weight == \"norm\":\n",
    "            max_dist = results_df[\"distance\"].max()\n",
    "            results_df[\"distance\"] = np.where(\n",
    "                results_df[\"distance\"]==0, 0, \n",
    "                max_dist - results_df[\"distance\"] + 1)\n",
    "        if preprocess_weight == \"recip\":\n",
    "            results_df[\"distance\"] = np.where(\n",
    "                results_df[\"distance\"]==0, 0, \n",
    "                1.0 / results_df[\"distance\"])\n",
    "    msps_df = results_df.groupby(\"source\").mean()\n",
    "    msps = msps_df.to_dict()[\"distance\"]\n",
    "    sorted_msps = sorted([(k, msps[k]) for k in msps.keys()], key=operator.itemgetter(1))\n",
    "    recos = [x[0] for x in sorted_msps[0:NUM_SENTS_IN_SUMMARY]]\n",
    "    return recos\n",
    "\n",
    "\n",
    "msp_recos = compute_mean_shortest_path(graph)\n",
    "display_recommendations(sent_index, \"MSP\", msp_recos)\n",
    "msp_n_recos = compute_mean_shortest_path(graph, preprocess_weight=\"norm\")\n",
    "display_recommendations(sent_index, \"MSP_n\", msp_n_recos)\n",
    "msp_r_recos = compute_mean_shortest_path(graph, preprocess_weight=\"recip\")\n",
    "display_recommendations(sent_index, \"MSP_r\", msp_n_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d-Ring Summarizer (CN-Ring)\n",
    "\n",
    "In this technique, we first choose the node with highest degree centrality, then add nodes to this subgraph by decreasing the degree centrality requirement in a stepped manner. At each stage, we add the newly qualified nodes to our subgraph. The process continues until we have reached the maximum number of sentences required by our summary.\n",
    "\n",
    "The intuition here is that we choose the most central nodes, proxies for the best summary sentences, from the graph in a step-wise manner. \n",
    "\n",
    "Two variants are proposed. The first one picks qualifying sentences by their position (earlier sentences are preferred), and the second picks qualifying sentences by their degree centrality. The correspond to the CN-Rings<sup>L</sup> and CN-Rings<sup>K</sup> respectively.\n",
    "\n",
    "Implementation wise, we have to generate a lookup table of node centrality, as well as a sorted list of nodes by centrality to support this and other similar methods later. This is done by the `compute_node_degrees` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ring_k: ['s005', 's021', 's056', 's062', 's063']\n",
      "    ring_l: ['s001', 's003', 's004', 's005', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_node_degrees(graph):\n",
    "    query = \"\"\"\n",
    "        CALL algo.degree.stream(\"Sentence\", \"SIM\", {direction: \"both\"})\n",
    "        YIELD nodeId, score\n",
    "        RETURN algo.asNode(nodeId).sid AS sid, score\n",
    "        ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    result = graph.run(query).data()\n",
    "    result_list = sorted([(x[\"sid\"], x[\"score\"]) for x in result],\n",
    "        key=operator.itemgetter(1), reverse=True)\n",
    "    result_dict = {x[\"sid\"]: x[\"score\"] for x in result}\n",
    "    return result_list, result_dict\n",
    "\n",
    "\n",
    "def _compute_rings(graph, cent_list, cent_dict, summary_sents, sort_by):\n",
    "    neighbor_sids = set()\n",
    "    for sid in summary_sents:\n",
    "        query = \"\"\"\n",
    "            MATCH (a {sid:\"%s\"})-[:SIM]->(b) \n",
    "            RETURN algo.getNodeById(id(b)).sid AS sid\n",
    "        \"\"\" % (sid)\n",
    "        result = graph.run(query).data()\n",
    "        neighbor_sids.update([x[\"sid\"] for x in result])\n",
    "    if sort_by == \"degree\":\n",
    "        neighbor_sids_scored = sorted([(x, cent_dict[x]) for x in neighbor_sids],\n",
    "            key=operator.itemgetter(1), reverse=True)\n",
    "        neighbor_sids = [x[0] for x in neighbor_sids_scored]\n",
    "    else:\n",
    "        neighbor_sids = sorted(neighbor_sids)\n",
    "    summary_sents.extend(neighbor_sids)\n",
    "    if len(summary_sents) >= NUM_SENTS_IN_SUMMARY:\n",
    "        return summary_sents\n",
    "    else:\n",
    "        _compute_rings(graph, cent_list, cent_dict, summary_sents, sort_by)\n",
    "\n",
    "        \n",
    "def compute_rings(graph, cent_list, cent_dict, sort_by):\n",
    "    summary_sents = [cent_list[0][0]]\n",
    "    _compute_rings(graph, cent_list, cent_dict, summary_sents, sort_by)\n",
    "    return sorted(summary_sents[0:NUM_SENTS_IN_SUMMARY])\n",
    "\n",
    "\n",
    "cent_list, cent_dict = compute_node_degrees(graph)\n",
    "\n",
    "ring_k_recos = compute_rings(graph, cent_list, cent_dict, sort_by=\"degree\")\n",
    "display_recommendations(sent_index, \"ring_k\", ring_k_recos)\n",
    "ring_l_recos = compute_rings(graph, cent_list, cent_dict, sort_by=\"position\")\n",
    "display_recommendations(sent_index, \"ring_l\", ring_l_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Core Summarizers (CN-Cores)\n",
    "\n",
    "Starts by choosing a small core consisting of node with highest degree centrality, then adding more nodes by sequentially decreasing the value of k. At each step, candidate nodes are added until the number of summary sentences is reached.\n",
    "\n",
    "Two variants are proposed, first adds candidate nodes by position, and second adds candidate nodes by centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    core_k: ['s005', 's021', 's056', 's062', 's063']\n",
      "    core_l: ['s005', 's021', 's056', 's062', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_cores(graph, result_list, result_dict, sort_by):\n",
    "    k_values = sorted(list(set([x[1] for x in result_list])), reverse=True)\n",
    "    summary_sents = []\n",
    "    for k_value in k_values:\n",
    "        new_sents = [x[0] for x in cent_list if x[1] == k_value]\n",
    "        if sort_by == \"degree\":\n",
    "            new_sents_scored = sorted([(x, cent_dict[x]) for x in new_sents],\n",
    "                key=operator.itemgetter(1), reverse=True)\n",
    "            new_sents = [x[0] for x in new_sents_scored]\n",
    "        else:\n",
    "            new_sents = sorted(new_sents)\n",
    "        summary_sents.extend(new_sents)\n",
    "        if len(summary_sents) > NUM_SENTS_IN_SUMMARY:\n",
    "            return sorted(summary_sents[0:NUM_SENTS_IN_SUMMARY])\n",
    "\n",
    "core_k_recos = compute_cores(graph, cent_list, cent_dict, sort_by=\"degree\")\n",
    "display_recommendations(sent_index, \"core_k\", core_k_recos)\n",
    "core_l_recos = compute_cores(graph, cent_list, cent_dict, sort_by=\"position\")\n",
    "display_recommendations(sent_index, \"core_l\", core_l_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w-Cuts Summarizers (CN-Cuts)\n",
    "\n",
    "Similar to k-cores, w-cuts starts with node pair with highest edge weight, and then progressively adds more edges by decreasing the edge weight threshold. At each stage, nodes from candidate triples are added to the summary, until the number of sentences for the summary are reached.\n",
    "\n",
    "Two variants are proposed, one where nodes are added based on position, and another where nodes are added based on degree. They correspond to CN-Cuts<sup>L</sup> and CN-Cuts<sup>K</sup> respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cut_k: ['s052', 's052', 's062', 's063', 's063']\n",
      "     cut_l: ['s052', 's052', 's062', 's063', 's063']\n"
     ]
    }
   ],
   "source": [
    "def compute_edge_weights(graph):\n",
    "    query = \"\"\"\n",
    "        MATCH (a)-[e:SIM]->(b) \n",
    "        RETURN a.sid AS source, e.sim AS weight, b.sid AS target \n",
    "        ORDER BY weight DESC\n",
    "    \"\"\"\n",
    "    result = graph.run(query).data()\n",
    "    edge_list = sorted([(x[\"source\"], x[\"weight\"], x[\"target\"]) for x in result],\n",
    "        key=operator.itemgetter(1), reverse=True)\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "def compute_cuts(graph, edge_list, cent_dict, sort_by):\n",
    "    w_values = sorted(list(set([x[1] for x in edge_list])), reverse=True)\n",
    "    summary_sents = []\n",
    "    for w_value in w_values:\n",
    "        node_pairs = [(x[0], x[2]) for x in edge_list if x[1] == w_value]\n",
    "        new_sents = []\n",
    "        for source, target in node_pairs:\n",
    "            if source not in summary_sents:\n",
    "                new_sents.append(source)\n",
    "            if target not in summary_sents:\n",
    "                new_sents.append(target)\n",
    "        if sort_by == \"degree\":\n",
    "            new_sents_scored = sorted([(x, cent_dict[x]) for x in new_sents],\n",
    "                key=operator.itemgetter(1), reverse=True)\n",
    "            new_sents = [x[0] for x in new_sents_scored]\n",
    "        else:\n",
    "            new_sents = sorted(new_sents)\n",
    "        summary_sents.extend(new_sents)\n",
    "        if len(summary_sents) > NUM_SENTS_IN_SUMMARY:\n",
    "            return sorted(summary_sents[0:NUM_SENTS_IN_SUMMARY])\n",
    "\n",
    "        \n",
    "edge_list = compute_edge_weights(graph)\n",
    "\n",
    "cut_k_recos = compute_cuts(graph, edge_list, cent_dict, sort_by=\"degree\")\n",
    "display_recommendations(sent_index, \"cut_k\", cut_k_recos)\n",
    "cut_l_recos = compute_cuts(graph, edge_list, cent_dict, sort_by=\"position\")\n",
    "display_recommendations(sent_index, \"cut_l\", cut_l_recos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting (CN-Voting)\n",
    "\n",
    "Recommendations from each summarizer are passed through a Majority Voting ensembling summarizer. The summarizer picks the most frequent sentence at each summary position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      vote: ['s005', 's021', 's056', 's062', 's063']\n",
      "s005 : Well before the Iranian leader's arrival in Caracas, his plans for a Latin America tour grabbed global attention as tensions grow between many Western powers and Iran over the nation's nuclear program.\n",
      "s021 : Spanish-language headlines on the network's website last week described Israeli spies, foreign intervention in Syria, a report that Japan plans to \"disobey\" U.S. sanctions against Iran and an allegation that airport security screening machines in the United States cause death.\n",
      "s056 : Officials in the United States and other Western nations have ratcheted up sanctions against Iran several times since a November report by the U.N. nuclear watchdog agency said the Iranian government was developing the technology needed to build a nuclear weapon.\n",
      "s062 : Some U.S. government officials and Washington analysts allege that Iran could be using new business relationships and energy deals with Latin American countries as a cover for more illicit projects, such as training Hezbollah militants and developing nuclear weapons.\n",
      "s063 : \"Iran and its Bolivarian allies (Venezuela, Bolivia, Nicaragua and Ecuador) in Latin America are systematically engaged in a pattern of financial behavior, recruitment exercises and business activities that are not economically rational and could be used for the movement and/or production of (weapons of mass destruction) and the furthering of Iran's stated aim of avoiding international sanctions on its nuclear program,\" Farah wrote in a May 2011 report for the U.S. Defense Department.\n"
     ]
    }
   ],
   "source": [
    "def find_most_frequent(votes):\n",
    "    vote_counter = collections.Counter()\n",
    "    for vote in votes:\n",
    "        vote_counter[vote] += 1\n",
    "    return vote_counter.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def compute_majority_vote(results):\n",
    "    lhs = None\n",
    "    for result in results:\n",
    "        if lhs is None:\n",
    "            lhs = result\n",
    "            continue\n",
    "        else:\n",
    "            rhs = result\n",
    "            assert(len(lhs) == len(rhs))\n",
    "    majority_recos = []\n",
    "    for i in range(len(lhs)):\n",
    "        recos_at_i = []\n",
    "        for result in results:\n",
    "            recos_at_i.append(result[i])\n",
    "        majority_recos.append(find_most_frequent(recos_at_i))\n",
    "    return majority_recos\n",
    "\n",
    "\n",
    "majvote_recos = compute_majority_vote([\n",
    "    degree_recos, strength_recos, closeness_recos, pagerank_recos,\n",
    "    msp_recos, msp_n_recos, msp_r_recos,\n",
    "    ring_k_recos, ring_l_recos, \n",
    "    core_k_recos, core_l_recos,\n",
    "    cut_k_recos, cut_l_recos\n",
    "])\n",
    "display_recommendations(sent_index, \"vote\", majvote_recos, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
