{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Story Files\n",
    "\n",
    "Data comes from the [DeepMind Question Answering Dataset](https://cs.nyu.edu/~kcho/DMQA/) consisting of CNN News Stories. Each story is packaged as a `.story` file in the dataset. A `.story` file consists of paragraphs of text for the story, followed by multiple manually created highlight phrases/sentences preceded by the `@highlight` annotation.\n",
    "\n",
    "The story file in our example [looks like this](../data/be8fc9fffd65a5a38153e83acc304a83c2e206e1.story).\n",
    "\n",
    "In this notebook, we will convert a specified `.story` file to two separate files -- one containing the sentences of the story text, one sentence per line, called `sentences.txt`, and another containing the lines of the highlights, called `summary.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORY_FILE = \"be8fc9fffd65a5a38153e83acc304a83c2e206e1.story\"\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "STORY_PATH = os.path.join(DATA_DIR, STORY_FILE)\n",
    "SENTENCE_PATH = os.path.join(DATA_DIR, \"sentences.txt\")\n",
    "SUMMARY_PATH = os.path.join(DATA_DIR, \"summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "We will use the sentence tokenizer from Spacy to tokenize input paragraph text into sentences.\n",
    "\n",
    "The `@highlight` annotations are at the end of the file, so when we see the first one, we set a flag and start writing exclusively to the summary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sent_tokenizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sent_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_sentences(line, nlp):\n",
    "    doc = nlp(line)\n",
    "    return [s.string for s in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstories = open(STORY_PATH, \"r\")\n",
    "fsentences = open(SENTENCE_PATH, \"w\")\n",
    "fsummary = open(SUMMARY_PATH, \"w\")\n",
    "\n",
    "start_summary = False\n",
    "\n",
    "for line in fstories:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    if line.startswith(\"@highlight\"):\n",
    "        start_summary = True\n",
    "        continue\n",
    "    if start_summary:\n",
    "        fsummary.write(line + \"\\n\")\n",
    "    else:\n",
    "        sents = tokenize_to_sentences(line, nlp)\n",
    "        for sent in sents:\n",
    "            fsentences.write(sent + \"\\n\")\n",
    "\n",
    "fsentences.close()\n",
    "fsummary.close()\n",
    "fstories.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files\n",
    "\n",
    "The `sentences.txt` file [looks like this](../data/sentences.txt).\n",
    "\n",
    "The `summary.txt` file [looks like this](../data/summary.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
