{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Newsgroup Posts\n",
    "\n",
    "Data for this example comes from the [20 Newsgroups Dataset](http://qwone.com/~jason/20Newsgroups/). This consists of approximately 18,800 newsgroup posts from 20 different neesgroups.\n",
    "\n",
    "The data is provided as a tar.gz file. Unzip and untar the file into a folder `data/20-newsgroups`. This will create a `20news-bydate-test` and `20news-bydate-train` folder below this directory. Under each of these two folders, additional folders corresponding to each of the 20 newsgroups contain the posting files, one file per post.\n",
    "\n",
    "This notebook will parse out the text from each post, regardless of folder, and write the text body for each post into a single line in the `texts.tsv` output file. We also capture the labels (name of newsgroup) into a `labels.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers(text_lines):\n",
    "    \"\"\" remove email headers -- email headers are separated from body\n",
    "        by a single empty line \"\"\"\n",
    "    for i in range(len(text_lines)):\n",
    "        if text_lines[i] == \"\\n\":\n",
    "            start = i + 1\n",
    "            break\n",
    "    return text_lines[start:]\n",
    "\n",
    "\n",
    "def preprocess_line(line):\n",
    "    \"\"\" line level preprocessing -- remove tabs and punctuation \"\"\"\n",
    "    line = line.strip()\n",
    "    line = line.replace('\\t', \" \")\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return line.translate(translator)\n",
    "\n",
    "\n",
    "def preprocess_tokens(tokens, stopwords):\n",
    "    \"\"\" token level preprocessing -- see steps \"\"\"\n",
    "    # remove quotes around characters\n",
    "    tokens = map(\n",
    "        lambda x: x[1:-1] if (x.startswith(\"'\") and x.endswith(\"'\")) else x, \n",
    "        tokens)\n",
    "    tokens = map(\n",
    "        lambda x: x[1:-1] if (x.startswith(\"\\\"\") and x.endswith(\"\\\"\")) else x, \n",
    "        tokens)\n",
    "    # remove numeric tokens\n",
    "    tokens = [x for x in tokens if not x.isdigit()]\n",
    "    # remove short words\n",
    "    tokens = [x for x in tokens if len(x) > 2]\n",
    "    # lowercase the tokens\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    # remove stopwords\n",
    "    tokens = [x for x in tokens if x not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def read_file_contents(file_path):\n",
    "    \"\"\" read file, preprocess text, return preprocessed text \"\"\"\n",
    "    with open(file_path, \"r\", errors=\"replace\") as fin:\n",
    "        text_lines = fin.readlines()\n",
    "    # remove headers\n",
    "    text_lines = remove_headers(text_lines)\n",
    "    tokens = []\n",
    "    for line in text_lines:\n",
    "        line = preprocess_line(line)\n",
    "        line_tokens = line.split(\" \")\n",
    "        line_tokens = preprocess_tokens(line_tokens, STOP_WORDS)\n",
    "        tokens.extend(line_tokens)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20news-bydate-test': 0, '20news-bydate-train': 1}\n",
      "{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}\n",
      "0 files processed\n",
      "1000 files processed\n",
      "2000 files processed\n",
      "3000 files processed\n",
      "4000 files processed\n",
      "5000 files processed\n",
      "6000 files processed\n",
      "7000 files processed\n",
      "8000 files processed\n",
      "9000 files processed\n",
      "10000 files processed\n",
      "11000 files processed\n",
      "12000 files processed\n",
      "13000 files processed\n",
      "14000 files processed\n",
      "15000 files processed\n",
      "16000 files processed\n",
      "17000 files processed\n",
      "18000 files processed\n",
      "18810 files processed, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "input_dir = os.path.join(DATA_DIR, \"20-newsgroups\")\n",
    "output_dir = os.path.join(DATA_DIR, \"docs\")\n",
    "\n",
    "subdir_lookup = {x:i for i, x in enumerate(os.listdir(input_dir))}\n",
    "print(subdir_lookup)\n",
    "subdir = list(subdir_lookup.keys())[0]\n",
    "label_lookup = {x:i for i, x in enumerate(os.listdir(os.path.join(input_dir, subdir)))}\n",
    "print(label_lookup)\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "num_written = 0\n",
    "ftexts = open(os.path.join(DATA_DIR, \"texts.tsv\"), \"w\")\n",
    "flabels = open(os.path.join(DATA_DIR, \"labels.tsv\"), \"w\")\n",
    "for subdir in os.listdir(input_dir):\n",
    "    for label in os.listdir(os.path.join(input_dir, subdir)):\n",
    "        for filename in os.listdir(os.path.join(input_dir, subdir, label)):\n",
    "            if num_written % 1000 == 0:\n",
    "                print(\"{:d} files processed\".format(num_written))\n",
    "            text = read_file_contents(os.path.join(input_dir, subdir, label, filename))\n",
    "            if len(text.strip()) == 0:\n",
    "                continue\n",
    "            doc_id = \"{:d}-{:d}-{:s}\".format(\n",
    "                subdir_lookup[subdir], label_lookup[label], filename)\n",
    "            ftexts.write(\"\\t\".join([doc_id, text]) + \"\\n\")\n",
    "            flabels.write(\"\\t\".join([doc_id, label]) + \"\\n\")\n",
    "            num_written += 1\n",
    "\n",
    "print(\"{:d} files processed, COMPLETE\".format(num_written))\n",
    "ftexts.close()\n",
    "flabels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
