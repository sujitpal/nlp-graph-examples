{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Similarity Matrix\n",
    "\n",
    "Documents are vectorized using Term Frequency / Inverse Document Frequency (TF-IDF) and stored in a Term-Document (TD) Matrix. Each document is represented by a vector of TF-IDF values. The size of the TF-IDF vector is equal to the size of the vocabulary.\n",
    "\n",
    "We then generate a similarity matrix S of cosine similarities by multiplying the (L2-normalized TD matrix with its transpose). Each entry of the similarity matrix S<sub>ij</sub> represents the similarity between document<sub>i</sub> and document<sub>j</sub>.\n",
    "\n",
    "The similarity matrix so formed will form the basis for creating **random walk** generation probabilities. In order to prevent self-loops, the diagonal of the similarity matrix is set to 0.\n",
    "\n",
    "This notebook contains the steps needed to build a TD matrix, and prepare and save the similarity matrix S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "TEXT_FILEPATH = os.path.join(DATA_DIR, \"texts.tsv\")\n",
    "\n",
    "TD_MATRIX_FILEPATH = os.path.join(DATA_DIR, \"tdmatrix.npz\")\n",
    "COSIM_FILEPATH = os.path.join(DATA_DIR, \"cosim.npy\")\n",
    "\n",
    "NUM_TOP_GENERATORS = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text into memory\n",
    "\n",
    "We will read the `texts.tsv` file and save the text into a local list object. We also save the doc_ids into a list, so we can correlate a row in the TD matrix with an actual `doc_id` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids, texts = [], []\n",
    "lid = 1\n",
    "ftext = open(TEXT_FILEPATH, \"r\")\n",
    "for line in ftext:\n",
    "    try:\n",
    "        doc_id, text = line.strip().split('\\t')\n",
    "    except ValueError:\n",
    "        print(\"line {:d}, num cols {:d}\".format(lid, len(line.strip().split('\\t'))))\n",
    "    doc_ids.append(doc_id)\n",
    "    texts.append(text)\n",
    "    lid += 1\n",
    "\n",
    "ftext.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and create TD Matrix\n",
    "\n",
    "We declare a Scikit-Learn TF-IDF Vectorizer with:\n",
    "* minimum document frequency -- token must appear in at least 5 documents to be counted in the vocabulary.\n",
    "* L2 normalization -- each row is normalized by the square root of the sum of its squared elements. \n",
    "\n",
    "L2 normalization is done in place because we are going to compute cosine similarity later.\n",
    "\n",
    "The TD matrix is a `scipy.sparse` matrix. We save it as an `.npz` file to use for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18810, 28813)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, norm=\"l2\")\n",
    "td_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "save_npz(TD_MATRIX_FILEPATH, td_matrix)\n",
    "\n",
    "print(td_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Similarity Matrix\n",
    "\n",
    "The TD Matrix represents a corpus of 18810 documents, each containing 28813 token features, or vectors of size 28813.\n",
    "\n",
    "We can generate a (18810, 18810) document-document similarity matrix by multiplying the TD matrix with its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18810, 18810)\n"
     ]
    }
   ],
   "source": [
    "S = td_matrix * np.transpose(td_matrix)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain only top generators\n",
    "\n",
    "We want to sparsify the similarity matrix by considering only the **top generators** (Kurland and Lee, 2005). The paper mentions that using 80 top generators gives good values downstream. \n",
    "\n",
    "So for each row (document), we will discard all elements except for the ones whose values are within the top 80 values for the row.\n",
    "\n",
    "Using `np.argpartitions` returns the top N values from a matrix along direction given by `axis`, but does not return them sorted. We don't need it sorted for our application, so thats fine. But using this is faster than the naive sorting and slicing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S.todense()\n",
    "num_to_discard = S.shape[1] - NUM_TOP_GENERATORS\n",
    "zero_indices = np.argpartition(S, -NUM_TOP_GENERATORS, axis=1)[:, 0:num_to_discard]\n",
    "for i in range(zero_indices.shape[0]):\n",
    "    for j in zero_indices[i]:\n",
    "        S[i, j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove self-loops\n",
    "\n",
    "The algorithm calls for generating random walks on the similarity graph, i.e., the graph generated by considering the similarity matrix S as an adjacency matrix. \n",
    "\n",
    "In this similarity graph, each node represents a document and each edge represents the probability of transitioning from the source document to the target document. The cosine similarity is expressed as a number between 0 and 1 and can be thought of as a proxy for this transition probability.\n",
    "\n",
    "We will execute random walks on this graph to get an estimate of the generation probabilities, i.e., what is the probability of being able to generate one document from another. We don't want to consider walks that start and end at the same node, as shown in the equation below.\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\fn_jvn&space;g(d_i&space;|&space;d_j)&space;=&space;\\left\\{\\begin{matrix}&space;0&space;&&space;if&space;\\,&space;i&space;=&space;j&space;\\\\&space;p(d_i&space;|&space;d_j)&space;&&space;otherwise&space;\\end{matrix}\\right.\" target=\"_blank\"><img src=\"https://latex.codecogs.com/png.latex?\\fn_jvn&space;g(d_i&space;|&space;d_j)&space;=&space;\\left\\{\\begin{matrix}&space;0&space;&&space;if&space;\\,&space;i&space;=&space;j&space;\\\\&space;p(d_i&space;|&space;d_j)&space;&&space;otherwise&space;\\end{matrix}\\right.\" title=\"g(d_i | d_j) = \\left\\{\\begin{matrix} 0 & if \\, i = j \\\\ p(d_i | d_j) & otherwise \\end{matrix}\\right.\" /></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(S.shape[0]):\n",
    "    S[i, i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renormalize Similarity Matrix\n",
    "\n",
    "In order for the resulting matrix to represent transition probabilities, we have to re-normalize the similarity matrix so the remaining elements sum to 1 across every row (unless all elements in the row are 0, in which case they sum to 0).\n",
    "\n",
    "We will save the similarity matrix for the next step in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/palsujit/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "S_rowsum = np.sum(S, axis=1).reshape(S.shape[0], 1)\n",
    "S_rowsum[S_rowsum == 0] = 1e-19\n",
    "Snorm = S / np.sum(S, axis=1)\n",
    "\n",
    "np.save(COSIM_FILEPATH, Snorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
